\section{Dimensionality reduction}
Dimensionality reduction can be used to analyze the extensive data generated by projecting the data into lower dimensions while minimizing the loss of information (such as a similarity measure, correlation between data points). 
For example, consider each data point to be an image of a ternary phase diagram represented in pixel coordinates \(\mathbb{R}^{4096}\). 
Our goal is to identify a lower dimensional embedding, preferably in two or three.

Principal Component Analysis~(PCA) is the most commonly used dimensionality reduction method that projects a data matrix \(X\in \mathbb{R}^{n \times D}\) of \(n\) data points in  with each row corresponding to a D-dimensional sample into lower dimensions. 
The goal in PCA is to find a linear projection \(Y\) of the samples into a dimension \(d\) that is much lower than original dimension \(D\) while maximizing variance of data represented by \(X\) in \(Y\). 
PCA can be performed on \(X\) using an equivalent dual formulation in terms of singular value decomposition of \(X\) after mean centering~\cite{ESL}. 
In essence, PCA simply finds a nested-sequences of \textit{linear} sub-spaces passing through the mean of the data.
Given \(n\) \(p\)-dimensional points \(x_1,x_2,\dots,x_N \in X\), PCA computes a rank-q linear map \(f(\lambda, \mu) = \mu + V_q\lambda\) parameterized by \(\lambda, \mu\). 
The linear map \(f(\lambda, \mu)\) is optimized to minimize the reconstruction error defined by~\Cref{eq:pcaloss}.
\begin{align}\label{eq:pcaloss}
    \mathcal{L}_{pca}:=\min_{\mu, {\lambda_i}, V_q} \sum_{i=1}^{N} \norm{x_i - \mu -V_q\lambda_i}
\end{align}
where \(\norm{.}\) is Euclidean norm. 
\Cref{eq:pcaloss} can be optimized by solving for \(\pdv{\mathcal{L}_{pca}}{\mu}=\pdv{\mathcal{L}_{pca}}{\lambda}=0\) and obtain~\Cref{eq:pcaoptims} with \(\bar{x} =\frac{1}{N} \sum_{i=1}^{N} x_i \)
\begin{align}
    \mu &= \bar{x} \nonumber \\
    \lambda &= V_{q}^{T}(x_i - \bar{x}) \label{eq:pcaoptims}
\end{align}
Substituting the optimized values of \(\mu, \lambda\) in~\Cref{eq:pcaloss}, we get the loss function in terms of only \(V_q\)~\Cref{eq:pcaeigprob} that can be solved using:
\begin{align}
    \mathcal{L}_{pca}&:=\min_{V_q} \sum_{i=1}^{N}\norm{(x_i - \bar{x}) - V_{q}V_{q}^{T}(x_i - \bar{x})} \nonumber \\
    &=\min_{V_q} \sum_{i=1}^{N}\norm{\Tilde{x_i} - V_{q}V_{q}^{T}\Tilde{x_i}} \label{eq:pcaeigprob}
\end{align}

i.e. for any given mean centered \(p\) dimensional vector \(\Tilde{x_i}\); the solution of \Cref{eq:pcaeigprob} is a \(V_q\) that projects \(\Tilde{x_i}\) orthogonally to a sub-space spanned by columns of \(V_q\).
Thus the above formulation is same as a \textit{singular value decomposition} of the data matrix \(X\in \mathbb{R}^{N\times p}\) as \(X = UDV^T\) and the first \(q\) columns of \(V\) is compose required \(V_q\). 

Typically, the notion of explained variance defined using the Eigen values of the covariance matrix is used to determine number of dimensions \(d\) for the target lower dimensional Euclidean space \(Y\).
This because, \Cref{eq:pcaeigprob} is also the SVD of \(S = XX^{T} = \frac{1}{N}VD^2V^T\).
For any Eigen vector \(e\) of \(S\) and we get the following:
\begin{align}
    e^TSe &= e^T (\mathbb{E}(XX^T)-\mathbb{E}(X)\mathbb{E}(X^T))e \nonumber \\
    &= \mathbb{E}((e^TX)(e^TX)^T)-\mathbb{E}(e^TX)\mathbb{E}(e^TX)^T) \nonumber \\
    &= \text{Var}(e^TX)
\end{align}

Note that \(\text{Var}(e^TX)e = ee^TSe = Se\) since Eigen vectors are orthonormal i.e. \(ee^T=I\).
Thus, the Eigen values of \(S\) are variance of data points projected along their respective Eigen vectors. 


When a vector like representation of data is not available but a notion of (dis)similarity is available, a Multi-dimensional Scaling~(MDS) approach is used. 
We assume that the similarity values define a metric space and would like to find an embedding of the unknown metric space into Euclidean such that the distances are same.
MDS allows us to find a lower dimensional embedding that preserves the pair-wise similarities between points defined a square similarity matrix~(\(S\)) to euclidean distances in the lower dimensions. 
Similar to PCA, MDS can also be formulated as a matrix decomposition problem when \(s_{ij}\in S\) represents \(D\)-dimensional Euclidean distance between two points \(i,j\)~\cite{ESL}. 
Suppose \(y_1,y_2,y_3,\dots,y_n \in \mathbb{R}^d\) be a lower dimensional coordinate representation of a high dimensional data in \(X\in \mathbb{R}^D\) for which have pari-wise similarity matrix \(S \in \mathbb{R}^{n \times n}\). 
The goal in MDS is to find a map \(f: Y \rightarrow X\) from observed data to preserve the pairwise similarities. The optimization problem of MDS is defined using a stress function~\cref{eq:mdsloss} that can be solved using a gradient descent algorithm starting from a random set of points for \(Y\in\mathbb{R}^d\).
\begin{align}\label{eq:mdsloss}
    \mathcal{L}_{mds}:= \sum_{i\neq j}(s_{ij} - \norm{z_i - z_j})^2
\end{align}
when the similarity matrix~\(S\) is composed of Euclidean norm between high-dimensional points, PCA and (classical)MDS are equivalent~\cite{ESL}.

Often times it is useful to make an assumption that we only want to capture the \textit{local} similarity in \(S\) especially when the target embedding the space is Euclidean.
This means that we assume that the metric space underlying \(S\) has a manifold structure (locally similar to flat Euclidean space but globally curved).
Popular methods that use manifold assumption for dimensionality reduction include Isomap~\cite{Isomap} and LLE~\cite{LLE}. 
The goal of Isomap algorithm is to preserve approximate manifold geodesics (i.e. the shortest paths between any two points) in lower dimensions by first constructing a neighborhood graph (such that we only care about local similarities) of the input data~(typically a D-dimensional point cloud) and approximating geodesics~\(g_G\) with shortest path distances between two points. 
MDS is then used to obtain a low dimensional embedding that preserves \(g_G\). 
Isomap approximates global geometry of the embedded manifold by mapping both nearby and faraway points to the respective counterparts in the desired lower dimensional embedding~\cite{IsomapV2}. 
Computing coordinate representation is similar to MDS but the map \(f\) is restricted to being an isometric embedding that preserves infinitesimal lengths and angles in sense of Riemannian geometry. 
A consequence of isometric restriction on \(f\) is that it preserves geodesics i.e. shortest paths between any two points in \(X \in\mathbb{R}^D\). 
Global geometry~(i.e. metric information) of \(X\) is computed by replacing the pairwise distances with shortest paths using a neighborhood graph~(such as k-nearest neighbor graph for a user defined k).
Note that explained variance for metric embedding methods presented above would be calculated using a correlation coefficient between the metric in \(M\) and Euclidean distance metric of \(Y\)~\cite{Isomap}.

\section{Clustering}
Another approach to understand large amounts data involves usage of clustering methods to identify sub-groups with close similarities to the points with in a group but highly dissimilar across. 
In this section, we provide a brief overview of various clustering methods. 
For detailed discussion on clustering analysis and algorithms, readers are referred to~\cite{ESL}. 

Similar to MDS, notion of similarity plays a key role in cluster analysis of a data set and defines membership (corresponding to a group or subset) criteria. 
We are interested in clustering data based on a pre-computed similarity between phase diagrams. 
We therefore review clustering method that work on using only a matrix of similarities given in \(S\) namely \textit{k-medoids} and \textit{spectral clustering}. 
\textit{k-medoids} clustering is a generalization of k-means clustering when the only data available is a similarity matrix \(S\). 
In contrast to k-means algorithm where mean observation points are used as cluster centers, \textit{k-medoids} aims to find k observed points that minimize with-in cluster dissimilarities~\cite{ESL}. 
In this thesis we used a \textit{spectral clustering} method where the clusters are associated with subsets of a graph that minimize variation of a function within in the subset and maximize across ~\cite{SpectralClustering}. 
The approach involves computing a graph Laplacian defined as \(L = D-A\) where \(D\) is degree matrix and \(A\) is the adjacency matrix obtained from \(S\).
Suppose we have a signal sampled at the nodes of the graph for an underlying function \(f\), we have the following identity:
\begin{align}
    f^T L f &= \frac{1}{2}\sum w_{ij}(f_i -f_j)^2 \nonumber \\
            &= \frac{1}{2}\sum \left(\frac{f_i -f_j}{s_{ij}}\right)^2 \\
\end{align}
The above identify should also motivate the name "Laplacian" and the its interpretation as a measure of variation of function \(f\) along the graph.
Spectral clustering involves computing Eigen values of \(L\) thus equivalent to performing Fourier analysis on graphs with Eigen values corresponding to the frequencies~\cite{GFT}. 
Other methods include density based clustering such as DBSCAN and its variants.  
